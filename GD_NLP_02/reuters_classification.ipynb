{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5340f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "079d7620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score #정확도 계산\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0e3bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "index_to_word = { index+3 : word for word, index in word_index.items() }\n",
    "# index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "  index_to_word[index]=token\n",
    "# print(' '.join([index_to_word[index] for index in x_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a5aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reuters_load_ml(num_words, mode = True):#mode true는 dtm, false는 tfidf\n",
    "    (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words, test_split=0.2)\n",
    "    decoded = []\n",
    "    for i in range(len(x_train)):\n",
    "        t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "        decoded.append(t)\n",
    "\n",
    "    x_train = decoded\n",
    "    decoded = []\n",
    "    for i in range(len(x_test)):\n",
    "        t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "        decoded.append(t)\n",
    "    x_test = decoded\n",
    "    dtmvector = CountVectorizer()\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    if mode :\n",
    "        x_train = dtmvector.fit_transform(x_train)\n",
    "        x_test = dtmvector.transform(x_test) #테스트 데이터를 DTM으로 변환\n",
    "    else :\n",
    "        x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "        x_train = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "        x_test_dtm = dtmvector.transform(x_test) #테스트 데이터를 DTM으로 변환\n",
    "        x_test = tfidf_transformer.transform(x_test_dtm) #DTM을 TF-IDF 행렬로 변환\n",
    "        \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dc1cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ml(x_train, y_train, x_test, y_test) :\n",
    "    #NB\n",
    "    model = MultinomialNB()\n",
    "    model.fit(x_train, y_train)\n",
    "    predicted = model.predict(x_test) #테스트 데이터에 대한 예측\n",
    "    print(\"NB 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    \n",
    "    #CNB\n",
    "    cb = ComplementNB()\n",
    "    cb.fit(x_train, y_train)\n",
    "    predicted = cb.predict(x_test) #테스트 데이터에 대한 예측\n",
    "    print(\"CNB 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    \n",
    "    #로지스틱회귀\n",
    "    lr = LogisticRegression(C=10000, penalty='l2', max_iter=3000)\n",
    "    lr.fit(x_train, y_train)\n",
    "    predicted = lr.predict(x_test) #테스트 데이터에 대한 예측\n",
    "    print(\"로지스틱회귀 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    \n",
    "    #svc\n",
    "    lsvc = LinearSVC(C=1000, penalty='l1', max_iter=3000, dual=False)\n",
    "    lsvc.fit(x_train, y_train)\n",
    "    predicted = lsvc.predict(x_test) #테스트 데이터에 대한 예측\n",
    "    print(\"SVC 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    \n",
    "    #tree\n",
    "    tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "    tree.fit(x_train, y_train)\n",
    "    predicted = tree.predict(x_test) #테스트 데이터에 대한 예측\n",
    "    print(\"tree 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    \n",
    "    #RandomForest\n",
    "    forest = RandomForestClassifier(n_estimators =5, random_state=0)\n",
    "    forest.fit(x_train, y_train)\n",
    "    predicted = forest.predict(x_test) #테스트 데이터에 대한 예측\n",
    "    print(\"RandomForest 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    \n",
    "    #GradientBoosting\n",
    "    grbt = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "    grbt.fit(x_train, y_train)\n",
    "    predicted = grbt.predict(x_test) #테스트 데이터에 대한 예측\n",
    "    print(\"GradientBoosting 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "\n",
    "    #보팅\n",
    "    voting_classifier = VotingClassifier(estimators=[('lr', lr), ('cb', cb), ('gnb', grbt)],voting='soft')\n",
    "    voting_classifier.fit(x_train, y_train)\n",
    "    predicted = voting_classifier.predict(x_test) #테스트 데이터에 대한 예측\n",
    "    print(\"보팅 정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "241dd655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words=None, DTM을 활용한 정확도\n",
      "NB 정확도: 0.7226179875333927\n",
      "CNB 정확도: 0.7782724844167409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로지스틱회귀 정확도: 0.7867319679430098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC 정확도: 0.7520035618878005\n",
      "tree 정확도: 0.6277827248441674\n",
      "RandomForest 정확도: 0.655387355298308\n",
      "GradientBoosting 정확도: 0.7711487088156723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보팅 정확도: 0.8116651825467498\n",
      "num_words=None, TFIDF을 활용한 정확도\n",
      "NB 정확도: 0.5997328584149599\n",
      "CNB 정확도: 0.7649154051647373\n",
      "로지스틱회귀 정확도: 0.8165627782724845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC 정확도: 0.7969723953695459\n",
      "tree 정확도: 0.6211041852181657\n",
      "RandomForest 정확도: 0.6544968833481746\n",
      "GradientBoosting 정확도: 0.7702582368655387\n",
      "보팅 정확도: 0.8156723063223509\n",
      "num_words=10000, DTM을 활용한 정확도\n",
      "NB 정확도: 0.7711487088156723\n",
      "CNB 정확도: 0.7773820124666073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로지스틱회귀 정확도: 0.780053428317008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC 정확도: 0.7466607301869991\n",
      "tree 정확도: 0.6273374888691006\n",
      "RandomForest 정확도: 0.6709706144256455\n",
      "GradientBoosting 정확도: 0.7724844167408726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보팅 정확도: 0.807212822796082\n",
      "num_words=10000, TFIDF을 활용한 정확도\n",
      "NB 정확도: 0.6567230632235085\n",
      "CNB 정확도: 0.7707034728406055\n",
      "로지스틱회귀 정확도: 0.8107747105966162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC 정확도: 0.7818343722172751\n",
      "tree 정확도: 0.6202137132680321\n",
      "RandomForest 정확도: 0.674087266251113\n",
      "GradientBoosting 정확도: 0.7662511130899377\n",
      "보팅 정확도: 0.8165627782724845\n",
      "num_words=5000, DTM을 활용한 정확도\n",
      "NB 정확도: 0.7773820124666073\n",
      "CNB 정확도: 0.7689225289403384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로지스틱회귀 정확도: 0.7778272484416741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC 정확도: 0.7252894033837934\n",
      "tree 정확도: 0.6242208370436332\n",
      "RandomForest 정확도: 0.6941228851291185\n",
      "GradientBoosting 정확도: 0.7702582368655387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보팅 정확도: 0.8107747105966162\n",
      "num_words=5000, TFIDF을 활용한 정확도\n",
      "NB 정확도: 0.6731967943009796\n",
      "CNB 정확도: 0.7707034728406055\n",
      "로지스틱회귀 정확도: 0.8036509349955476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC 정확도: 0.7707034728406055\n",
      "tree 정확도: 0.6179875333926982\n",
      "RandomForest 정확도: 0.701246660730187\n",
      "GradientBoosting 정확도: 0.767586821015138\n",
      "보팅 정확도: 0.8103294746215495\n"
     ]
    }
   ],
   "source": [
    "print(\"num_words=None, DTM을 활용한 정확도\") \n",
    "x_train, y_train, x_test, y_test = reuters_load_ml(None,True)\n",
    "fit_ml(x_train, y_train, x_test, y_test)\n",
    "print(\"num_words=None, TFIDF을 활용한 정확도\") \n",
    "x_train, y_train, x_test, y_test = reuters_load_ml(None,False)\n",
    "fit_ml(x_train, y_train, x_test, y_test)\n",
    "print(\"num_words=10000, DTM을 활용한 정확도\") \n",
    "x_train, y_train, x_test, y_test = reuters_load_ml(10000,True)\n",
    "fit_ml(x_train, y_train, x_test, y_test)\n",
    "print(\"num_words=10000, TFIDF을 활용한 정확도\") \n",
    "x_train, y_train, x_test, y_test = reuters_load_ml(10000,False)\n",
    "fit_ml(x_train, y_train, x_test, y_test)\n",
    "print(\"num_words=5000, DTM을 활용한 정확도\") \n",
    "x_train, y_train, x_test, y_test = reuters_load_ml(5000,True)\n",
    "fit_ml(x_train, y_train, x_test, y_test)\n",
    "print(\"num_words=5000, TFIDF을 활용한 정확도\") \n",
    "x_train, y_train, x_test, y_test = reuters_load_ml(5000,False)\n",
    "fit_ml(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f42bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, model.predict(x_test_dtm), zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e489a98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def graph_confusion_matrix(model, x_test, y_test):#, classes_name):\n",
    "  df_cm = pd.DataFrame(confusion_matrix(y_test, model.predict(x_test)))#, index=classes_name, columns=classes_name)\n",
    "  fig = plt.figure(figsize=(15,15))\n",
    "  heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "  heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=12)\n",
    "  heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=12)\n",
    "  plt.ylabel('label')\n",
    "  plt.xlabel('predicted value')\n",
    "\n",
    "# graph_confusion_matrix(model, x_test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29d5a0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "281/281 [==============================] - 6s 10ms/step - loss: 2.2924 - accuracy: 0.4035 - val_loss: 1.8512 - val_accuracy: 0.5236\n",
      "Epoch 2/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 1.7453 - accuracy: 0.5371 - val_loss: 1.7439 - val_accuracy: 0.5610\n",
      "Epoch 3/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 1.6135 - accuracy: 0.5824 - val_loss: 1.6635 - val_accuracy: 0.5841\n",
      "Epoch 4/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 1.3987 - accuracy: 0.6407 - val_loss: 1.4774 - val_accuracy: 0.6394\n",
      "Epoch 5/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 1.1476 - accuracy: 0.7104 - val_loss: 1.4116 - val_accuracy: 0.6589\n",
      "Epoch 6/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.9576 - accuracy: 0.7571 - val_loss: 1.3207 - val_accuracy: 0.6670\n",
      "Epoch 7/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.7836 - accuracy: 0.7944 - val_loss: 1.3717 - val_accuracy: 0.6541\n",
      "Epoch 8/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.6474 - accuracy: 0.8326 - val_loss: 1.3158 - val_accuracy: 0.7017\n",
      "Epoch 9/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.5423 - accuracy: 0.8598 - val_loss: 1.3453 - val_accuracy: 0.6830\n",
      "Epoch 10/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.4352 - accuracy: 0.8914 - val_loss: 1.3528 - val_accuracy: 0.7035\n",
      "Epoch 11/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.3738 - accuracy: 0.9070 - val_loss: 1.4109 - val_accuracy: 0.7012\n",
      "Epoch 12/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.3119 - accuracy: 0.9224 - val_loss: 1.4623 - val_accuracy: 0.6919\n",
      "Epoch 13/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.2698 - accuracy: 0.9330 - val_loss: 1.4786 - val_accuracy: 0.6981\n",
      "Epoch 14/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.2223 - accuracy: 0.9426 - val_loss: 1.5828 - val_accuracy: 0.6995\n",
      "Epoch 15/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.2017 - accuracy: 0.9447 - val_loss: 1.5994 - val_accuracy: 0.7026\n",
      "Epoch 16/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.1816 - accuracy: 0.9459 - val_loss: 1.6226 - val_accuracy: 0.6915\n",
      "Epoch 17/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.1647 - accuracy: 0.9501 - val_loss: 1.6100 - val_accuracy: 0.7093\n",
      "Epoch 18/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.1521 - accuracy: 0.9523 - val_loss: 1.7018 - val_accuracy: 0.6883\n",
      "Epoch 19/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.1376 - accuracy: 0.9520 - val_loss: 1.7154 - val_accuracy: 0.6959\n",
      "Epoch 20/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.1351 - accuracy: 0.9522 - val_loss: 1.7725 - val_accuracy: 0.6919\n",
      "Epoch 21/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.1286 - accuracy: 0.9539 - val_loss: 1.8156 - val_accuracy: 0.6803\n",
      "Epoch 22/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.1199 - accuracy: 0.9538 - val_loss: 1.7690 - val_accuracy: 0.6999\n",
      "Epoch 23/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.1161 - accuracy: 0.9525 - val_loss: 1.8249 - val_accuracy: 0.6972\n",
      "Epoch 24/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.1055 - accuracy: 0.9540 - val_loss: 1.8183 - val_accuracy: 0.7030\n",
      "Epoch 25/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.1020 - accuracy: 0.9536 - val_loss: 1.8694 - val_accuracy: 0.7017\n",
      "Epoch 26/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.1007 - accuracy: 0.9566 - val_loss: 1.8399 - val_accuracy: 0.7021\n",
      "Epoch 27/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.1045 - accuracy: 0.9536 - val_loss: 1.8528 - val_accuracy: 0.6941\n",
      "Epoch 28/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.0958 - accuracy: 0.9538 - val_loss: 1.9504 - val_accuracy: 0.6923\n",
      "Epoch 29/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.0933 - accuracy: 0.9567 - val_loss: 1.8657 - val_accuracy: 0.6995\n",
      "Epoch 30/30\n",
      "281/281 [==============================] - 2s 8ms/step - loss: 0.0894 - accuracy: 0.9554 - val_loss: 1.9785 - val_accuracy: 0.6888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.42      0.43        12\n",
      "           1       0.71      0.60      0.65       105\n",
      "           2       0.64      0.45      0.53        20\n",
      "           3       0.86      0.92      0.89       813\n",
      "           4       0.80      0.76      0.78       474\n",
      "           5       0.33      0.20      0.25         5\n",
      "           6       0.57      0.57      0.57        14\n",
      "           7       1.00      0.33      0.50         3\n",
      "           8       0.61      0.61      0.61        38\n",
      "           9       0.70      0.64      0.67        25\n",
      "          10       0.92      0.80      0.86        30\n",
      "          11       0.53      0.46      0.49        83\n",
      "          12       0.36      0.31      0.33        13\n",
      "          13       0.23      0.30      0.26        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.25      0.11      0.15         9\n",
      "          16       0.48      0.66      0.55        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.32      0.45      0.38        20\n",
      "          19       0.61      0.50      0.55       133\n",
      "          20       0.48      0.40      0.44        70\n",
      "          21       0.21      0.26      0.23        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.18      0.33      0.24        12\n",
      "          24       0.43      0.16      0.23        19\n",
      "          25       0.76      0.42      0.54        31\n",
      "          26       0.57      0.50      0.53         8\n",
      "          27       0.29      0.50      0.36         4\n",
      "          28       0.11      0.30      0.16        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.27      0.33      0.30        12\n",
      "          31       0.11      0.08      0.09        13\n",
      "          32       0.42      0.50      0.45        10\n",
      "          33       0.67      0.80      0.73         5\n",
      "          34       0.17      0.14      0.15         7\n",
      "          35       0.25      0.17      0.20         6\n",
      "          36       0.26      0.55      0.35        11\n",
      "          37       0.50      0.50      0.50         2\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.40      0.20      0.27        10\n",
      "          41       0.00      0.00      0.00         8\n",
      "          42       0.00      0.00      0.00         3\n",
      "          43       0.56      0.83      0.67         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.69      2246\n",
      "   macro avg       0.40      0.39      0.38      2246\n",
      "weighted avg       0.69      0.69      0.69      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Reuters 데이터셋 로드\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000, test_split=0.2)\n",
    "# 데이터 전처리\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_train = label_binarizer.fit_transform(y_train)\n",
    "y_test = label_binarizer.fit_transform(y_test)\n",
    "# 시퀀스 패딩\n",
    "max_sequence_length = 100  # 시퀀스의 최대 길이 지정\n",
    "x_train = pad_sequences(x_train, maxlen=max_sequence_length)\n",
    "x_test = pad_sequences(x_test, maxlen=max_sequence_length)\n",
    "# 모델 구성\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=100, input_length=max_sequence_length))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(units=46, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_test, y_test))\n",
    "\n",
    "# 테스트 데이터에 대한 예측\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# 분류 보고서 출력\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "report = classification_report(y_test_labels, y_pred_labels)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
